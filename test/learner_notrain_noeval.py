import gc
import os
import time

import cloudpickle
from pebble import ProcessPool

import metisfl.proto.proto_messages_factory as proto_factory
from metisfl.learner.grpc_controller_client import GRPCControllerClient
from metisfl.models.model_dataset import (ModelDataset,
                                          ModelDatasetClassification,
                                          ModelDatasetRegression)
from metisfl.proto import learner_pb2, metis_pb2, model_pb2
from metisfl.utils.logger import MetisLogger


class Learner(object):
    """
    Any invocation to the public functions of the Learner instance need to be wrapped inside a process,
    since the body of every function is generating a new Neural Network registry/context.

    In order to be able to run the training/evaluation/prediction functions as independent
    processes, we needed to create a ModelOperations class factory, depending on the neural network engine
    being used. For instance, for Keras we define the `_keras_model_ops_factory()` that internally imports
    the KerasModelOps class and for PyTorch we follow the same design by importing the PyTorchModeOps
    inside the body of the `_pytorch_model_ops_factory()` function.

    Specifically, no ModelOps subclass should be imported in the global scope of the learner but rather
    within the local scope (i.e., namespace) of each neural network model operations factory function.
    """

    def __init__(self, learner_server_entity: metis_pb2.ServerEntity,
                 controller_server_entity: metis_pb2.ServerEntity,
                 he_scheme: metis_pb2.HEScheme, nn_engine, model_fp,
                 train_dataset_fp, train_dataset_recipe_pkl,
                 validation_dataset_fp="", validation_dataset_recipe_pkl="",
                 test_dataset_fp="", test_dataset_recipe_pkl="",
                 recreate_queue_task_worker=False,
                 learner_credentials_fp="/tmp/metis/learner/"):
        self._learner_server_entity = learner_server_entity
        self._controller_server_entity = controller_server_entity
        self._he_scheme = he_scheme
        self._nn_engine = nn_engine
        self._model_fp = model_fp

        assert train_dataset_fp is not None and train_dataset_recipe_pkl is not None
        self.train_dataset_recipe_pkl, self.train_dataset_fp = \
            train_dataset_recipe_pkl, train_dataset_fp
        self.validation_dataset_recipe_pkl, self.validation_dataset_fp = \
            validation_dataset_recipe_pkl, validation_dataset_fp
        self.test_dataset_recipe_pkl, self.test_dataset_fp = \
            test_dataset_recipe_pkl, test_dataset_fp

        self._learner_controller_client = GRPCControllerClient(
            self._controller_server_entity, max_workers=1)
        # The `learner_id` param is generated by the controller with the join federation request
        # and it is used thereafter for every incoming/forwarding request.
        self.__learner_credentials_fp = learner_credentials_fp
        if not os.path.exists(self.__learner_credentials_fp):
            os.mkdir(self.__learner_credentials_fp)
        self.__learner_id = None
        self.__auth_token = None
        # TODO(stripeli): if we want to be more secure, we can dump an
        #  encrypted version of auth_token and learner_id
        self.__learner_id_fp = os.path.join(self.__learner_credentials_fp, "learner_id.txt")
        self.__auth_token_fp = os.path.join(self.__learner_credentials_fp, "auth_token.txt")

    def __getstate__(self):
        """
        Python needs to pickle the entire object, including its instance variables.
        Since one of these variables is the Pool object itself, the entire object cannot be pickled.
        We need to remove the Pool() variable from the object state in order to use the pool_task.
        The same holds for the gprc client, which underlying uses a futures thread pool.
        See also: https://stackoverflow.com/questions/25382455
        """
        self_dict = self.__dict__.copy()
        del self_dict['_learner_controller_client']
        return self_dict

    def _create_model_dataset_helper(self, dataset_recipe_pkl, dataset_fp, default_class=None):
        if dataset_recipe_pkl and dataset_fp:
            dataset_recipe_fn = cloudpickle.load(open(dataset_recipe_pkl, "rb"))
            dataset = dataset_recipe_fn(dataset_fp)
            assert isinstance(dataset, ModelDataset)
        else:
            # If the validation or testing dataset have not been specified,
            # then we use as default class the training dataset's class.
            dataset = default_class()
        return dataset

    def _load_model_datasets(self):
        train_dataset = self._create_model_dataset_helper(
            self.train_dataset_recipe_pkl, self.train_dataset_fp)
        validation_dataset = self._create_model_dataset_helper(
            self.validation_dataset_recipe_pkl, self.validation_dataset_fp,
            default_class=train_dataset.__class__)
        test_dataset = self._create_model_dataset_helper(
            self.test_dataset_recipe_pkl, self.test_dataset_fp,
            default_class=train_dataset.__class__)
        return train_dataset, validation_dataset, test_dataset

    def _load_model_datasets_size_specs_type_def(self):
        # Load only the dataset size, specifications and class type because
        # numpys or tf.tensors cannot be serialized and hence cannot be returned through the process.
        return [(d.get_size(), d.get_model_dataset_specifications(), type(d)) for d in self._load_model_datasets()]

    def _load_datasets_metadata_subproc(self):
        _generic_tasks_pool = ProcessPool(max_workers=1, max_tasks=1)
        datasets_specs_future = _generic_tasks_pool.schedule(function=self._load_model_datasets_size_specs_type_def)
        res = datasets_specs_future.result()
        _generic_tasks_pool.close()
        _generic_tasks_pool.join()
        return res

    def _mark_learning_task_completed(self, training_future):
        # If the returned future was completed successfully and was not cancelled,
        # meaning it did complete its running job, then notify the controller.
        if training_future.done() and not training_future.cancelled():
            completed_task_pb = training_future.result()
            self._learner_controller_client.mark_task_completed(
                learner_id=self.__learner_id,
                auth_token=self.__auth_token,
                completed_task_pb=completed_task_pb,
                block=False)

    def host_port_identifier(self):
        return "{}:{}".format(
            self._learner_server_entity.hostname,
            self._learner_server_entity.port)

    def join_federation(self):
        MetisLogger.info(
            "Delaying learner: {} join federation request by 30 seconds".format(self.host_port_identifier()))
        time.sleep(30)
        # FIXME(stripeli): If we create a learner controller instance
        #  once (without channel initialization) then the program hangs!
        train_dataset_meta, validation_dataset_meta, test_dataset_meta = self._load_datasets_metadata_subproc()
        is_classification = train_dataset_meta[2] == ModelDatasetClassification
        is_regression = train_dataset_meta[2] == ModelDatasetRegression

        self.__learner_id, self.__auth_token, status = \
            self._learner_controller_client.join_federation(self._learner_server_entity,
                                                            self.__learner_id_fp,
                                                            self.__auth_token_fp,
                                                            train_dataset_meta[0],
                                                            train_dataset_meta[1],
                                                            validation_dataset_meta[0],
                                                            validation_dataset_meta[1],
                                                            test_dataset_meta[0],
                                                            test_dataset_meta[1],
                                                            is_classification,
                                                            is_regression)
        return status

    def leave_federation(self):
        status = self._learner_controller_client.leave_federation(self.__learner_id, self.__auth_token, block=False)
        # Make sure that all pending tasks have been processed.
        self._learner_controller_client.shutdown()
        return status

    def run_evaluation_task(self, model_pb: model_pb2.Model, batch_size: int,
                            evaluation_dataset_pb: [learner_pb2.EvaluateModelRequest.dataset_to_eval],
                            metrics_pb, cancel_running_tasks=False, block=False, verbose=False):
        time.sleep(3)
        return metis_pb2.ModelEvaluations()

    def run_inference_task(self):
        time.sleep(3)
        raise NotImplementedError("Not yet implemented.")

    def run_learning_task(self, learning_task_pb: metis_pb2.LearningTask,
                          hyperparameters_pb: metis_pb2.Hyperparameters, model_pb: model_pb2.Model,
                          cancel_running_tasks=False, block=False, verbose=False):
        time.sleep(3)
        task_evaluation_pb = \
            proto_factory.MetisProtoMessages.construct_task_evaluation_pb(
                epoch_training_evaluations_pbs=[],
                epoch_validation_evaluations_pbs=[],
                epoch_test_evaluations_pbs=[])
        completed_learning_task = metis_pb2.CompletedLearningTask(
            model=model_pb,
            execution_metadata=proto_factory.MetisProtoMessages.construct_task_execution_metadata_pb(
                learning_task_pb.global_iteration, task_evaluation_pb, 1, 100, 1, 100.0, 100.0),
            aux_metadata="")
        while self.__learner_id is None:
            time.sleep(1)
        self._learner_controller_client.mark_task_completed(
            learner_id=self.__learner_id,
            auth_token=self.__auth_token,
            completed_task_pb=completed_learning_task,
            block=False)
        return True

    def shutdown(self,
                 cancel_train_running_tasks=True,
                 cancel_eval_running_tasks=True,
                 cancel_infer_running_tasks=True):
        gc.collect()
        return True
